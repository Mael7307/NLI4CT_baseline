{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d54f89-71ed-4d69-990b-1f3512a982b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9639aeb-4ac3-4c82-8821-e67fac34de57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>evidences</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There were only 3 adverse events in NCT0089450...</td>\n",
       "      <td>[\"Adverse Events 1:\", \"  Total: 10/71 (14.08%)...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>compared to cohort 1 of NCT00475670, there are...</td>\n",
       "      <td>[\"Adverse Events 1:\", \"  Total: 0/3 (0.00%)\", ...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There were 0 observed cases of Tibia or Fibula...</td>\n",
       "      <td>[\"Adverse Events 1:\", \"  Total: 16/149 (10.74%...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There 55 more Participants With Best Tumor Res...</td>\n",
       "      <td>[\"Outcome Measurement: \", \"  Percentage of Par...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women classified as high-risk of developing br...</td>\n",
       "      <td>[\"Inclusion Criteria:\", \"  Gail risk &gt;= 1.7% a...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>There were 4 cases of Febrile neutropenia and ...</td>\n",
       "      <td>[\"Adverse Events 1:\", \"  Febrile neutropenia 4...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>At least 1 patient in NCT00022516 suffered fro...</td>\n",
       "      <td>[\"Adverse Events 1:\", \"  Total: 0/0\", \"  Leuko...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Every patient in the Palbociclib+Letrozole Aus...</td>\n",
       "      <td>[\"Outcome Measurement: \", \"  Number of Partici...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Patients currently prescribed Diuretics are ex...</td>\n",
       "      <td>[\"Exclusion Criteria:\", \"  medication(s) known...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>In contrast to NCT00768222, NCT02102490 did no...</td>\n",
       "      <td>[\"Adverse Events 1:\", \"  Total: 33/132 (25.00%...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           hypothesis  \\\n",
       "0   There were only 3 adverse events in NCT0089450...   \n",
       "1   compared to cohort 1 of NCT00475670, there are...   \n",
       "2   There were 0 observed cases of Tibia or Fibula...   \n",
       "3   There 55 more Participants With Best Tumor Res...   \n",
       "4   Women classified as high-risk of developing br...   \n",
       "..                                                ...   \n",
       "75  There were 4 cases of Febrile neutropenia and ...   \n",
       "76  At least 1 patient in NCT00022516 suffered fro...   \n",
       "77  Every patient in the Palbociclib+Letrozole Aus...   \n",
       "78  Patients currently prescribed Diuretics are ex...   \n",
       "79  In contrast to NCT00768222, NCT02102490 did no...   \n",
       "\n",
       "                                            evidences          label  \n",
       "0   [\"Adverse Events 1:\", \"  Total: 10/71 (14.08%)...  Contradiction  \n",
       "1   [\"Adverse Events 1:\", \"  Total: 0/3 (0.00%)\", ...     Entailment  \n",
       "2   [\"Adverse Events 1:\", \"  Total: 16/149 (10.74%...  Contradiction  \n",
       "3   [\"Outcome Measurement: \", \"  Percentage of Par...  Contradiction  \n",
       "4   [\"Inclusion Criteria:\", \"  Gail risk >= 1.7% a...     Entailment  \n",
       "..                                                ...            ...  \n",
       "75  [\"Adverse Events 1:\", \"  Febrile neutropenia 4...  Contradiction  \n",
       "76  [\"Adverse Events 1:\", \"  Total: 0/0\", \"  Leuko...     Entailment  \n",
       "77  [\"Outcome Measurement: \", \"  Number of Partici...     Entailment  \n",
       "78  [\"Exclusion Criteria:\", \"  medication(s) known...     Entailment  \n",
       "79  [\"Adverse Events 1:\", \"  Total: 33/132 (25.00%...  Contradiction  \n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df=pd.read_csv('./output/dev_hypothesis_evidences.csv')\n",
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212df86b-c0f6-4965-bca5-ad4966cf78e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5d8b363-c68c-4562-bb3d-8770d7c8f418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_lst=dev_df['hypothesis'].values.tolist()\n",
    "len(hypothesis_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8efe8157-ae5d-41c2-834b-27b33b160b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence_lst=dev_df['evidences'].apply(lambda l:' '.join(json.loads(l))).values.tolist()\n",
    "len(evidence_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8414a700-6dce-4600-bf9e-348083904d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id={\"Contradiction\":0,\"Entailment\":1}\n",
    "label_lst=dev_df['label'].apply(lambda x:label2id[x]).values.tolist()\n",
    "len(label_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc36c52-1423-46cf-a7ca-2e024248c536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b616df7-cc19-42fe-8d96-21a98ebe369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55163bb8-916d-474d-a970-418a7415fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# text_tok=AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# text_clf=AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "#roberta-base\n",
    "text_tok=AutoTokenizer.from_pretrained('roberta-base')\n",
    "text_clf=AutoModelForSequenceClassification.from_pretrained('roberta-base',num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73236413-f29a-4f46-b22e-2fd86a9232e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "975670e9-455c-4dd2-87b3-59b53c19514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputSequence:\n",
    "    \n",
    "    def __init__(self,tok,l_text,l_text2,l_label,batch_size=64,gpu=True):\n",
    "        \n",
    "        self.data_len=len(l_text)\n",
    "        self.data_idx=[i for i in range(self.data_len)]\n",
    "        self.texts=tok(l_text,l_text2,padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        self.l_label=np.array(l_label)\n",
    "        print('tokenize done')\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        self.gpu=gpu\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.data_idx)\n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "        start=i*self.batch_size\n",
    "        batch_idx=self.data_idx[start:min(start+self.batch_size,self.data_len)]\n",
    "        \n",
    "        return_texts=dict([(k,self.texts[k][batch_idx]) for k in self.texts])\n",
    "        return_labels=torch.from_numpy(\n",
    "            self.l_label[batch_idx].astype(np.int64)\n",
    "        )\n",
    "        \n",
    "        if self.gpu:\n",
    "            return_texts=dict([(k,return_texts[k].cuda()) for k in return_texts])\n",
    "            return_labels=return_labels.cuda()\n",
    "        \n",
    "        return return_texts,return_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(1.0*self.data_len/self.batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e900de28-95f2-4e4b-8a54-cbc530d83edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d69c714-4ee8-4189-8236-644d83ad93ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize done\n"
     ]
    }
   ],
   "source": [
    "testing_data=InputSequence(text_tok,hypothesis_lst,evidence_lst,label_lst,gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84262558-9f7d-40f1-b67d-ad91ddf809da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ./output/clf_models/roberta-base_epoch_00009.pt batch: 1\r"
     ]
    }
   ],
   "source": [
    "scores=[]\n",
    "# model_names=['bert-base-uncased']+[\n",
    "#     './output/clf_models/bert-base-uncased_epoch_{}.pt'.format(format(epoch,'05d'))\n",
    "#     for epoch in range(10)\n",
    "# ]\n",
    "#roberta-base\n",
    "model_names=['roberta-base']+[\n",
    "    './output/clf_models/roberta-base_epoch_{}.pt'.format(format(epoch,'05d'))\n",
    "    for epoch in range(10)\n",
    "]\n",
    "for model_name in model_names:\n",
    "    scores.append([])\n",
    "    clf=AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n",
    "    with torch.no_grad():\n",
    "        for batch in range(len(testing_data)):\n",
    "            batch_texts,batch_labels=testing_data[batch]\n",
    "            scores[-1].append(F.softmax(clf(**batch_texts).logits,dim=1).detach().cpu().numpy())\n",
    "            print('model:',model_name,'batch:',batch,end='\\r')\n",
    "    scores[-1]=np.concatenate(scores[-1],axis=0)\n",
    "    clf.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e35e51be-bc41-4165-9499-f813c31d65bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>AVG_PREC</th>\n",
       "      <th>F1</th>\n",
       "      <th>PREC</th>\n",
       "      <th>REC</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pretrained</td>\n",
       "      <td>0.525703</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.445184</td>\n",
       "      <td>0.655462</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.4875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.434511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.426259</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.423549</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.466932</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.493506</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.4750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.413002</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.407725</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.3375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.383538</td>\n",
       "      <td>0.365591</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.2625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.384274</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.3250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.367605</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.2875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         epoch  AVG_PREC        F1      PREC       REC     ACC\n",
       "0   pretrained  0.525703  0.677686  0.512500  1.000000  0.5125\n",
       "1            1  0.445184  0.655462  0.500000  0.951220  0.4875\n",
       "2            2  0.434511  0.000000  0.000000  0.000000  0.4750\n",
       "3            3  0.426259  0.241379  0.411765  0.170732  0.4500\n",
       "4            4  0.423549  0.117647  0.300000  0.073171  0.4375\n",
       "5            5  0.466932  0.644068  0.493506  0.926829  0.4750\n",
       "6            6  0.413002  0.297297  0.333333  0.268293  0.3500\n",
       "7            7  0.407725  0.495238  0.406250  0.634146  0.3375\n",
       "8            8  0.383538  0.365591  0.326923  0.414634  0.2625\n",
       "9            9  0.384274  0.386364  0.361702  0.414634  0.3250\n",
       "10          10  0.367605  0.240000  0.264706  0.219512  0.2875"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score,f1_score,precision_score,recall_score,accuracy_score\n",
    "\n",
    "y_true=label_lst\n",
    "results=[]\n",
    "for epoch in range(len(scores)):\n",
    "    y_prob=scores[epoch][:,1]\n",
    "    y_pred=[1 if a>0.5 else 0 for a in y_prob]\n",
    "    results.append([\n",
    "        'pretrained' if epoch==0 else epoch,\n",
    "        average_precision_score(y_true,y_prob),\n",
    "        f1_score(y_true,y_pred),\n",
    "        precision_score(y_true,y_pred),\n",
    "        recall_score(y_true,y_pred),\n",
    "        accuracy_score(y_true,y_pred)\n",
    "    ])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(results,columns=['epoch','AVG_PREC','F1','PREC','REC','ACC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8265fd-2878-420e-866f-6761190fba30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
