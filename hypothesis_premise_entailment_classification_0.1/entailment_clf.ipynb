{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>evidences</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patients with cytologically confirmed breast c...</td>\n",
       "      <td>[\"Inclusion Criteria:\", \"  Histologically or c...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Women with leptomeningeal metastases are unfor...</td>\n",
       "      <td>[\"  Exclusion Criteria\", \"  Patients with know...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cohort 1 of NCT02513472 produced better Object...</td>\n",
       "      <td>[\"Outcome Measurement: \", \"  Objective Respons...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT01237327 and NCT00030823 do not have any ov...</td>\n",
       "      <td>[\"Inclusion Criteria:\", \"  Previous participat...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>More than 2% of patients in NCT00129376 experi...</td>\n",
       "      <td>[\"Adverse Events 1:\", \"  Acute Pharyngitis * 1...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>NCT00696072 uses Overall Survival (OS) as outc...</td>\n",
       "      <td>[\"Outcome Measurement: \", \"  Proportion of Par...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>NCT00320541 recorded more cardiac related adve...</td>\n",
       "      <td>[\"Adverse Events 1:\", \"  Total: 17/50 (34.00%)...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>There were 2 cases of Angina in NCT02370238.</td>\n",
       "      <td>[\"Adverse Events 1:\", \"  Total: 13/61 (21.31%)...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>NCT00645333 records several central nervous sy...</td>\n",
       "      <td>[\"  Autoimmune disorder *1/30 (3.33%)\", \"  Imm...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>Cohort 1 of NCT00493636 had a longer median, m...</td>\n",
       "      <td>[\"Outcome Measurement: \", \"  Progression Free ...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            hypothesis  \\\n",
       "0    Patients with cytologically confirmed breast c...   \n",
       "1    Women with leptomeningeal metastases are unfor...   \n",
       "2    Cohort 1 of NCT02513472 produced better Object...   \n",
       "3    NCT01237327 and NCT00030823 do not have any ov...   \n",
       "4    More than 2% of patients in NCT00129376 experi...   \n",
       "..                                                 ...   \n",
       "715  NCT00696072 uses Overall Survival (OS) as outc...   \n",
       "716  NCT00320541 recorded more cardiac related adve...   \n",
       "717       There were 2 cases of Angina in NCT02370238.   \n",
       "718  NCT00645333 records several central nervous sy...   \n",
       "719  Cohort 1 of NCT00493636 had a longer median, m...   \n",
       "\n",
       "                                             evidences          label  \n",
       "0    [\"Inclusion Criteria:\", \"  Histologically or c...  Contradiction  \n",
       "1    [\"  Exclusion Criteria\", \"  Patients with know...  Contradiction  \n",
       "2    [\"Outcome Measurement: \", \"  Objective Respons...     Entailment  \n",
       "3    [\"Inclusion Criteria:\", \"  Previous participat...     Entailment  \n",
       "4    [\"Adverse Events 1:\", \"  Acute Pharyngitis * 1...  Contradiction  \n",
       "..                                                 ...            ...  \n",
       "715  [\"Outcome Measurement: \", \"  Proportion of Par...  Contradiction  \n",
       "716  [\"Adverse Events 1:\", \"  Total: 17/50 (34.00%)...  Contradiction  \n",
       "717  [\"Adverse Events 1:\", \"  Total: 13/61 (21.31%)...  Contradiction  \n",
       "718  [\"  Autoimmune disorder *1/30 (3.33%)\", \"  Imm...  Contradiction  \n",
       "719  [\"Outcome Measurement: \", \"  Progression Free ...     Entailment  \n",
       "\n",
       "[720 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=pd.read_csv('./output/train_hypothesis_evidences.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_lst=train_df['hypothesis'].values.tolist()\n",
    "len(hypothesis_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence_lst=train_df['evidences'].apply(lambda l:' '.join(json.loads(l))).values.tolist()\n",
    "len(evidence_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id={\"Contradiction\":0,\"Entailment\":1}\n",
    "label_lst=train_df['label'].apply(lambda x:label2id[x]).values.tolist()\n",
    "len(label_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputSequence:\n",
    "    \n",
    "    def __init__(self,tok,l_text,l_text2,l_label,batch_size=64,gpu=True):\n",
    "        \n",
    "        self.data_len=len(l_text)\n",
    "        self.data_idx=[i for i in range(self.data_len)]\n",
    "        self.texts=tok(l_text,l_text2,padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        self.l_label=np.array(l_label)\n",
    "        print('tokenize done')\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        self.gpu=gpu\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.data_idx)\n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "        start=i*self.batch_size\n",
    "        batch_idx=self.data_idx[start:min(start+self.batch_size,self.data_len)]\n",
    "        \n",
    "        return_texts=dict([(k,self.texts[k][batch_idx]) for k in self.texts])\n",
    "        return_labels=torch.from_numpy(\n",
    "            self.l_label[batch_idx].astype(np.int64)\n",
    "        )\n",
    "        \n",
    "        if self.gpu:\n",
    "            return_texts=dict([(k,return_texts[k].cuda()) for k in return_texts])\n",
    "            return_labels=return_labels.cuda()\n",
    "        \n",
    "        return return_texts,return_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(1.0*self.data_len/self.batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a263f04c4fb24a81abd71e13b74b27b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b3bb0b59f64588b84dd1183284fa43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3b3bd43a1240369a3d6d02246db926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c72718d8de47119bc86d3d77df2c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3937b59781548d786cce6471fa5f34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# text_tok=AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# text_clf=AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "#roberta-base\n",
    "text_tok=AutoTokenizer.from_pretrained('roberta-base')\n",
    "text_clf=AutoModelForSequenceClassification.from_pretrained('roberta-base',num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data=InputSequence(text_tok,hypothesis_lst,evidence_lst,label_lst,gpu=True)\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,clf):\n",
    "        super(Model, self).__init__()\n",
    "        self.clf=clf\n",
    "        self.loss=nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, texts, labels, gpu=True):\n",
    "        \n",
    "        loss=self.loss(self.clf(**texts).logits, labels)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(text_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 loss: 0.6977713108062744\n",
      "epoch: 0 batch: 22 loss: 0.7047663927078247\n",
      "epoch: 1 batch: 0 loss: 0.6914390325546265\n",
      "epoch: 1 batch: 22 loss: 0.6853869557380676\n",
      "epoch: 2 batch: 0 loss: 0.6950498223304749\n",
      "epoch: 2 batch: 22 loss: 0.6986944675445557\n",
      "epoch: 3 batch: 0 loss: 0.6923381686210632\n",
      "epoch: 3 batch: 22 loss: 0.7052376270294193\n",
      "epoch: 4 batch: 0 loss: 0.672080934047699\n",
      "epoch: 4 batch: 22 loss: 0.6746116280555725\n",
      "epoch: 5 batch: 0 loss: 0.7033475041389465\n",
      "epoch: 5 batch: 22 loss: 0.6757913231849671\n",
      "epoch: 6 batch: 0 loss: 0.6580055356025696\n",
      "epoch: 6 batch: 22 loss: 0.6612647771835327\n",
      "epoch: 7 batch: 0 loss: 0.6295934915542603\n",
      "epoch: 7 batch: 22 loss: 0.5723339915275574\n",
      "epoch: 8 batch: 0 loss: 0.531297504901886\n",
      "epoch: 8 batch: 22 loss: 0.5323455929756165\n",
      "epoch: 9 batch: 0 loss: 0.4005983769893646\n",
      "epoch: 9 batch: 22 loss: 0.52865719795227056\n"
     ]
    }
   ],
   "source": [
    "bat_s=32\n",
    "l_rate=1e-5\n",
    "\n",
    "training_data.batch_size=bat_s\n",
    "\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_rate)\n",
    "total_epoch_num=10\n",
    "for epoch in range(total_epoch_num):\n",
    "    training_data.on_epoch_end()\n",
    "    loss_sum=0.0\n",
    "    loss_count=0\n",
    "    for batch in range(len(training_data)):\n",
    "        optimizer.zero_grad()\n",
    "        batch_texts,batch_labels=training_data[batch]\n",
    "        loss_count+=len(batch_texts['input_ids'])\n",
    "        loss = model(\n",
    "            batch_texts,batch_labels\n",
    "        )\n",
    "        print('epoch:',epoch,'batch:',batch,'loss:',loss.item(),end='\\n' if batch==0 or batch+1==len(training_data) or (batch+1)%1000==0 else '\\r')\n",
    "        loss_sum += 1.0*loss.item()*len(batch_texts['input_ids'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # model.clf.save_pretrained('./output/clf_models/bert-base-uncased_epoch_{}.pt'.format(format(epoch,'05d')))\n",
    "    #roberta-base\n",
    "    model.clf.save_pretrained('./output/clf_models/roberta-base_epoch_{}.pt'.format(format(epoch,'05d')))\n",
    "_=model.cpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
