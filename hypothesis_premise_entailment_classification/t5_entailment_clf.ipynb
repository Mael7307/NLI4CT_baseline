{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>evidences</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All the primary trial participants do not rece...</td>\n",
       "      <td>[\"INTERVENTION 1: \", \"  Diagnostic (FLT PET)\",...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Patients with Platelet count over 100,000/mm¬¨...</td>\n",
       "      <td>[\"  PATIENT CHARACTERISTICS:\", \"  ANC  1,500/m...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heart-related adverse events were recorded in ...</td>\n",
       "      <td>[\"Adverse Events 1:\", \"  Supraventricular tach...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adult Patients with histologic confirmation of...</td>\n",
       "      <td>[\"Inclusion Criteria:\", \"  Patients with histo...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Laser Therapy is in each cohort of the primary...</td>\n",
       "      <td>[\"INTERVENTION 1: \", \"  Laser Therapy Alone\", ...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>Adequate blood, kidney, and hepatic function a...</td>\n",
       "      <td>[\"Inclusion Criteria:\", \"  Postmenopausal wome...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>The Ridaforolimus + Dalotuzumab + Exemestane g...</td>\n",
       "      <td>[\"Outcome Measurement: \", \"  1. Progression-fr...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>The only difference between the interventions ...</td>\n",
       "      <td>[\"INTERVENTION 1: \", \"  Prone\", \"Prone positio...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>Patients must have a white blood cell count ab...</td>\n",
       "      <td>[\"  WBC &gt; 1,500/mm\\u00b3\"]</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>the primary trial and the secondary trial both...</td>\n",
       "      <td>[\"Outcome Measurement: \", \"  Central Nervous S...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             hypothesis  \\\n",
       "0     All the primary trial participants do not rece...   \n",
       "1     Patients with Platelet count over 100,000/mm¬¨...   \n",
       "2     Heart-related adverse events were recorded in ...   \n",
       "3     Adult Patients with histologic confirmation of...   \n",
       "4     Laser Therapy is in each cohort of the primary...   \n",
       "...                                                 ...   \n",
       "1695  Adequate blood, kidney, and hepatic function a...   \n",
       "1696  The Ridaforolimus + Dalotuzumab + Exemestane g...   \n",
       "1697  The only difference between the interventions ...   \n",
       "1698  Patients must have a white blood cell count ab...   \n",
       "1699  the primary trial and the secondary trial both...   \n",
       "\n",
       "                                              evidences          label  \n",
       "0     [\"INTERVENTION 1: \", \"  Diagnostic (FLT PET)\",...  Contradiction  \n",
       "1     [\"  PATIENT CHARACTERISTICS:\", \"  ANC  1,500/m...  Contradiction  \n",
       "2     [\"Adverse Events 1:\", \"  Supraventricular tach...     Entailment  \n",
       "3     [\"Inclusion Criteria:\", \"  Patients with histo...  Contradiction  \n",
       "4     [\"INTERVENTION 1: \", \"  Laser Therapy Alone\", ...  Contradiction  \n",
       "...                                                 ...            ...  \n",
       "1695  [\"Inclusion Criteria:\", \"  Postmenopausal wome...     Entailment  \n",
       "1696  [\"Outcome Measurement: \", \"  1. Progression-fr...  Contradiction  \n",
       "1697  [\"INTERVENTION 1: \", \"  Prone\", \"Prone positio...     Entailment  \n",
       "1698                         [\"  WBC > 1,500/mm\\u00b3\"]     Entailment  \n",
       "1699  [\"Outcome Measurement: \", \"  Central Nervous S...  Contradiction  \n",
       "\n",
       "[1700 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=pd.read_csv('./output/train_hypothesis_evidences.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1700"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_lst=train_df['hypothesis'].values.tolist()\n",
    "len(hypothesis_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1700"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence_lst=train_df['evidences'].apply(lambda l:' '.join(json.loads(l))).values.tolist()\n",
    "len(evidence_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1700"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id={\"Contradiction\":0,\"Entailment\":1}\n",
    "label_lst=train_df['label'].apply(lambda x:label2id[x]).values.tolist()\n",
    "len(label_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputSequence:\n",
    "    \n",
    "    def __init__(self,tok,l_text,l_text2,l_label,batch_size=64,gpu=True,task_prefix = \"Detect entailment: \"):\n",
    "        \n",
    "        self.data_len=len(l_text)\n",
    "        self.data_idx=[i for i in range(self.data_len)]\n",
    "        self.texts=tok([task_prefix+' '.join([a,b]) for a,b in zip(l_text,l_text2)],\n",
    "                       padding=\"longest\",\n",
    "                       max_length=512,#max_source_length,\n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\",\n",
    "                      )\n",
    "        self.labels=tok([\"Entailment\" if lab==1 else \"Contradiction\" for lab in l_label],\n",
    "                         padding=\"longest\",\n",
    "                         max_length=128,#,\n",
    "                         truncation=True,\n",
    "                         return_tensors=\"pt\",\n",
    "                        )\n",
    "        print('tokenize done')\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        self.gpu=gpu\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.data_idx)\n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "        start=i*self.batch_size\n",
    "        batch_idx=self.data_idx[start:min(start+self.batch_size,self.data_len)]\n",
    "        \n",
    "        return_texts=dict([(k,self.texts[k][batch_idx]) for k in self.texts])\n",
    "        return_labels=dict([(k,self.labels[k][batch_idx]) for k in self.labels])\n",
    "        \n",
    "        if self.gpu:\n",
    "            return_texts=dict([(k,return_texts[k].cuda()) for k in return_texts])\n",
    "            return_labels=dict([(k,return_labels[k].cuda()) for k in return_labels])\n",
    "        \n",
    "        return return_texts,return_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(1.0*self.data_len/self.batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x20779zz/.local/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "text_tok = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "text_clf = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data=InputSequence(text_tok,hypothesis_lst,evidence_lst,label_lst,gpu=True)\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,clf):\n",
    "        super(Model, self).__init__()\n",
    "        self.clf=clf\n",
    "        # self.loss=nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, texts, labels, gpu=True):\n",
    "        \n",
    "        input_ids, attention_mask = texts['input_ids'], texts['attention_mask']\n",
    "        labels = labels['input_ids']\n",
    "        # replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "        labels[labels == text_tok.pad_token_id] = -100\n",
    "        \n",
    "        loss=self.clf(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(text_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 loss: 7.865568161010742\n",
      "epoch: 0 batch: 53 loss: 1.4363217353820884\n",
      "epoch: 1 batch: 0 loss: 0.8371789455413818\n",
      "epoch: 1 batch: 53 loss: 0.20250956714153296\n",
      "epoch: 2 batch: 0 loss: 0.2891809940338135\n",
      "epoch: 2 batch: 53 loss: 0.18026919662952423\n",
      "epoch: 3 batch: 0 loss: 0.24582313001155853\n",
      "epoch: 3 batch: 53 loss: 0.27415791153907776\n",
      "epoch: 4 batch: 0 loss: 0.20681999623775482\n",
      "epoch: 4 batch: 53 loss: 0.20907138288021088\n",
      "epoch: 5 batch: 0 loss: 0.20660193264484406\n",
      "epoch: 5 batch: 53 loss: 0.27594047784805367\n",
      "epoch: 6 batch: 0 loss: 0.19517120718955994\n",
      "epoch: 6 batch: 53 loss: 0.25715890526771545\n",
      "epoch: 7 batch: 0 loss: 0.20219534635543823\n",
      "epoch: 7 batch: 53 loss: 0.15706640481948853\n",
      "epoch: 8 batch: 0 loss: 0.1940225064754486\n",
      "epoch: 8 batch: 53 loss: 0.22445161640644073\n",
      "epoch: 9 batch: 0 loss: 0.19233182072639465\n",
      "epoch: 9 batch: 53 loss: 0.13230130076408386\n"
     ]
    }
   ],
   "source": [
    "bat_s=32\n",
    "l_rate=1e-5\n",
    "\n",
    "training_data.batch_size=bat_s\n",
    "\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_rate)\n",
    "total_epoch_num=10\n",
    "for epoch in range(total_epoch_num):\n",
    "    training_data.on_epoch_end()\n",
    "    loss_sum=0.0\n",
    "    loss_count=0\n",
    "    for batch in range(len(training_data)):\n",
    "        optimizer.zero_grad()\n",
    "        batch_texts,batch_labels=training_data[batch]\n",
    "        loss_count+=len(batch_texts['input_ids'])\n",
    "        loss = model(\n",
    "            batch_texts,batch_labels\n",
    "        )\n",
    "        print('epoch:',epoch,'batch:',batch,'loss:',loss.item(),end='\\n' if batch==0 or batch+1==len(training_data) or (batch+1)%1000==0 else '\\r')\n",
    "        loss_sum += 1.0*loss.item()*len(batch_texts['input_ids'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #T5\n",
    "    model.clf.save_pretrained('./output/clf_models/t5-base_epoch_{}.pt'.format(format(epoch,'05d')))\n",
    "_=model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the following 2 hyperparameters are task-specific\n",
    "max_source_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "# Suppose we have the following 2 training examples:\n",
    "input_sequence_1 = \"Welcome to NYC\"\n",
    "output_sequence_1 = \"Bienvenue à NYC\"\n",
    "\n",
    "input_sequence_2 = \"HuggingFace is a company\"\n",
    "output_sequence_2 = \"HuggingFace est une entreprise\"\n",
    "\n",
    "# encode the inputs\n",
    "task_prefix = \"Detect entailment: \"\n",
    "input_sequences = [input_sequence_1, input_sequence_2]\n",
    "\n",
    "encoding = tokenizer(\n",
    "    [task_prefix + sequence for sequence in input_sequences],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_source_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "\n",
    "# encode the targets\n",
    "target_encoding = tokenizer(\n",
    "    [output_sequence_1, output_sequence_2],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_target_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "labels = target_encoding.input_ids\n",
    "\n",
    "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "# forward pass\n",
    "loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "loss.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
