{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d54f89-71ed-4d69-990b-1f3512a982b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9639aeb-4ac3-4c82-8821-e67fac34de57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>evidences</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patients with cytologically confirmed breast c...</td>\n",
       "      <td>[\"Inclusion Criteria:\", \"  Histologically or c...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Women with leptomeningeal metastases are unfor...</td>\n",
       "      <td>[\"  Exclusion Criteria\", \"  Patients with know...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cohort 1 of NCT02513472 produced better Object...</td>\n",
       "      <td>[\"Outcome Measurement: \", \"  Objective Respons...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT01237327 and NCT00030823 do not have any ov...</td>\n",
       "      <td>[\"Inclusion Criteria:\", \"  Previous participat...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>More than 2% of patients in NCT00129376 experi...</td>\n",
       "      <td>[\"Adverse Events 1:\", \"  Acute Pharyngitis * 1...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>NCT00696072 uses Overall Survival (OS) as outc...</td>\n",
       "      <td>[\"Outcome Measurement: \", \"  Proportion of Par...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>NCT00320541 recorded more cardiac related adve...</td>\n",
       "      <td>[\"Adverse Events 1:\", \"  Total: 17/50 (34.00%)...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>There were 2 cases of Angina in NCT02370238.</td>\n",
       "      <td>[\"Adverse Events 1:\", \"  Total: 13/61 (21.31%)...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>NCT00645333 records several central nervous sy...</td>\n",
       "      <td>[\"  Autoimmune disorder *1/30 (3.33%)\", \"  Imm...</td>\n",
       "      <td>Contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>Cohort 1 of NCT00493636 had a longer median, m...</td>\n",
       "      <td>[\"Outcome Measurement: \", \"  Progression Free ...</td>\n",
       "      <td>Entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            hypothesis  \\\n",
       "0    Patients with cytologically confirmed breast c...   \n",
       "1    Women with leptomeningeal metastases are unfor...   \n",
       "2    Cohort 1 of NCT02513472 produced better Object...   \n",
       "3    NCT01237327 and NCT00030823 do not have any ov...   \n",
       "4    More than 2% of patients in NCT00129376 experi...   \n",
       "..                                                 ...   \n",
       "715  NCT00696072 uses Overall Survival (OS) as outc...   \n",
       "716  NCT00320541 recorded more cardiac related adve...   \n",
       "717       There were 2 cases of Angina in NCT02370238.   \n",
       "718  NCT00645333 records several central nervous sy...   \n",
       "719  Cohort 1 of NCT00493636 had a longer median, m...   \n",
       "\n",
       "                                             evidences          label  \n",
       "0    [\"Inclusion Criteria:\", \"  Histologically or c...  Contradiction  \n",
       "1    [\"  Exclusion Criteria\", \"  Patients with know...  Contradiction  \n",
       "2    [\"Outcome Measurement: \", \"  Objective Respons...     Entailment  \n",
       "3    [\"Inclusion Criteria:\", \"  Previous participat...     Entailment  \n",
       "4    [\"Adverse Events 1:\", \"  Acute Pharyngitis * 1...  Contradiction  \n",
       "..                                                 ...            ...  \n",
       "715  [\"Outcome Measurement: \", \"  Proportion of Par...  Contradiction  \n",
       "716  [\"Adverse Events 1:\", \"  Total: 17/50 (34.00%)...  Contradiction  \n",
       "717  [\"Adverse Events 1:\", \"  Total: 13/61 (21.31%)...  Contradiction  \n",
       "718  [\"  Autoimmune disorder *1/30 (3.33%)\", \"  Imm...  Contradiction  \n",
       "719  [\"Outcome Measurement: \", \"  Progression Free ...     Entailment  \n",
       "\n",
       "[720 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df=pd.read_csv('./output/train_hypothesis_evidences.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212df86b-c0f6-4965-bca5-ad4966cf78e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5d8b363-c68c-4562-bb3d-8770d7c8f418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_lst=test_df['hypothesis'].values.tolist()\n",
    "len(hypothesis_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8efe8157-ae5d-41c2-834b-27b33b160b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence_lst=test_df['evidences'].apply(lambda l:' '.join(json.loads(l))).values.tolist()\n",
    "len(evidence_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8414a700-6dce-4600-bf9e-348083904d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id={\"Contradiction\":0,\"Entailment\":1}\n",
    "label_lst=test_df['label'].apply(lambda x:label2id[x]).values.tolist()\n",
    "len(label_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc36c52-1423-46cf-a7ca-2e024248c536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b616df7-cc19-42fe-8d96-21a98ebe369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55163bb8-916d-474d-a970-418a7415fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# text_tok=AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# text_clf=AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "#roberta-base\n",
    "text_tok=AutoTokenizer.from_pretrained('roberta-base')\n",
    "text_clf=AutoModelForSequenceClassification.from_pretrained('roberta-base',num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73236413-f29a-4f46-b22e-2fd86a9232e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "975670e9-455c-4dd2-87b3-59b53c19514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputSequence:\n",
    "    \n",
    "    def __init__(self,tok,l_text,l_text2,l_label,batch_size=64,gpu=True):\n",
    "        \n",
    "        self.data_len=len(l_text)\n",
    "        self.data_idx=[i for i in range(self.data_len)]\n",
    "        self.texts=tok(l_text,l_text2,padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        self.l_label=np.array(l_label)\n",
    "        print('tokenize done')\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        self.gpu=gpu\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.data_idx)\n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "        start=i*self.batch_size\n",
    "        batch_idx=self.data_idx[start:min(start+self.batch_size,self.data_len)]\n",
    "        \n",
    "        return_texts=dict([(k,self.texts[k][batch_idx]) for k in self.texts])\n",
    "        return_labels=torch.from_numpy(\n",
    "            self.l_label[batch_idx].astype(np.int64)\n",
    "        )\n",
    "        \n",
    "        if self.gpu:\n",
    "            return_texts=dict([(k,return_texts[k].cuda()) for k in return_texts])\n",
    "            return_labels=return_labels.cuda()\n",
    "        \n",
    "        return return_texts,return_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(1.0*self.data_len/self.batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e900de28-95f2-4e4b-8a54-cbc530d83edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d69c714-4ee8-4189-8236-644d83ad93ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize done\n"
     ]
    }
   ],
   "source": [
    "testing_data=InputSequence(text_tok,hypothesis_lst,evidence_lst,label_lst,gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84262558-9f7d-40f1-b67d-ad91ddf809da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ./output/clf_models/roberta-base_epoch_00009.pt batch: 11\r"
     ]
    }
   ],
   "source": [
    "scores=[]\n",
    "# model_names=['bert-base-uncased']+[\n",
    "#     './output/clf_models/bert-base-uncased_epoch_{}.pt'.format(format(epoch,'05d'))\n",
    "#     for epoch in range(10)\n",
    "# ]\n",
    "#roberta-base\n",
    "model_names=['roberta-base']+[\n",
    "    './output/clf_models/roberta-base_epoch_{}.pt'.format(format(epoch,'05d'))\n",
    "    for epoch in range(10)\n",
    "]\n",
    "for model_name in model_names:\n",
    "    scores.append([])\n",
    "    clf=AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\n",
    "    with torch.no_grad():\n",
    "        for batch in range(len(testing_data)):\n",
    "            batch_texts,batch_labels=testing_data[batch]\n",
    "            scores[-1].append(F.softmax(clf(**batch_texts).logits,dim=1).detach().cpu().numpy())\n",
    "            print('model:',model_name,'batch:',batch,end='\\r')\n",
    "    scores[-1]=np.concatenate(scores[-1],axis=0)\n",
    "    clf.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e35e51be-bc41-4165-9499-f813c31d65bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>AVG_PREC</th>\n",
       "      <th>F1</th>\n",
       "      <th>PREC</th>\n",
       "      <th>REC</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pretrained</td>\n",
       "      <td>0.491515</td>\n",
       "      <td>0.661710</td>\n",
       "      <td>0.494444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.494444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.563004</td>\n",
       "      <td>0.658467</td>\n",
       "      <td>0.496434</td>\n",
       "      <td>0.977528</td>\n",
       "      <td>0.498611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.576840</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.019663</td>\n",
       "      <td>0.513889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.585947</td>\n",
       "      <td>0.421818</td>\n",
       "      <td>0.597938</td>\n",
       "      <td>0.325843</td>\n",
       "      <td>0.558333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.600798</td>\n",
       "      <td>0.407547</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.303371</td>\n",
       "      <td>0.563889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.623830</td>\n",
       "      <td>0.675024</td>\n",
       "      <td>0.513950</td>\n",
       "      <td>0.983146</td>\n",
       "      <td>0.531944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.671782</td>\n",
       "      <td>0.618557</td>\n",
       "      <td>0.650155</td>\n",
       "      <td>0.589888</td>\n",
       "      <td>0.640278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.753473</td>\n",
       "      <td>0.714579</td>\n",
       "      <td>0.563107</td>\n",
       "      <td>0.977528</td>\n",
       "      <td>0.613889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.827803</td>\n",
       "      <td>0.766055</td>\n",
       "      <td>0.647287</td>\n",
       "      <td>0.938202</td>\n",
       "      <td>0.716667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.868962</td>\n",
       "      <td>0.829517</td>\n",
       "      <td>0.758140</td>\n",
       "      <td>0.915730</td>\n",
       "      <td>0.813889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.946616</td>\n",
       "      <td>0.871942</td>\n",
       "      <td>0.893805</td>\n",
       "      <td>0.851124</td>\n",
       "      <td>0.876389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         epoch  AVG_PREC        F1      PREC       REC       ACC\n",
       "0   pretrained  0.491515  0.661710  0.494444  1.000000  0.494444\n",
       "1            1  0.563004  0.658467  0.496434  0.977528  0.498611\n",
       "2            2  0.576840  0.038462  0.875000  0.019663  0.513889\n",
       "3            3  0.585947  0.421818  0.597938  0.325843  0.558333\n",
       "4            4  0.600798  0.407547  0.620690  0.303371  0.563889\n",
       "5            5  0.623830  0.675024  0.513950  0.983146  0.531944\n",
       "6            6  0.671782  0.618557  0.650155  0.589888  0.640278\n",
       "7            7  0.753473  0.714579  0.563107  0.977528  0.613889\n",
       "8            8  0.827803  0.766055  0.647287  0.938202  0.716667\n",
       "9            9  0.868962  0.829517  0.758140  0.915730  0.813889\n",
       "10          10  0.946616  0.871942  0.893805  0.851124  0.876389"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score,f1_score,precision_score,recall_score,accuracy_score\n",
    "\n",
    "y_true=label_lst\n",
    "results=[]\n",
    "for epoch in range(len(scores)):\n",
    "    y_prob=scores[epoch][:,1]\n",
    "    y_pred=[1 if a>0.5 else 0 for a in y_prob]\n",
    "    results.append([\n",
    "        'pretrained' if epoch==0 else epoch,\n",
    "        average_precision_score(y_true,y_prob),\n",
    "        f1_score(y_true,y_pred),\n",
    "        precision_score(y_true,y_pred),\n",
    "        recall_score(y_true,y_pred),\n",
    "        accuracy_score(y_true,y_pred)\n",
    "    ])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(results,columns=['epoch','AVG_PREC','F1','PREC','REC','ACC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8265fd-2878-420e-866f-6761190fba30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
